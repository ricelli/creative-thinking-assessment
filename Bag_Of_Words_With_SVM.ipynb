{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbu0yXTWqc87"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from multiprocessing import Pool\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "import time\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_train = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')"
      ],
      "metadata": {
        "id": "AgV4n4CeyuQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.sort_index()"
      ],
      "metadata": {
        "id": "Dgzw3_cC0jlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')"
      ],
      "metadata": {
        "id": "8QB54kXI5VsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.sort_index()"
      ],
      "metadata": {
        "id": "qFl839pH5b0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.sample()['response']"
      ],
      "metadata": {
        "id": "BbFMBFmS-UbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(text, lang, domain_stopwords=[]):\n",
        "\n",
        "    stop_words = nltk.corpus.stopwords.words(portuguese) # lang='portuguese' or lang='english'\n",
        "\n",
        "    # Transforma em caixa baixa\n",
        "    s = str(text).lower()\n",
        "\n",
        "    # Remove pontuação\n",
        "    table = str.maketrans({key: None for key in string.punctuation.replace(\"'\", \"\")})\n",
        "    s = s.translate(table)\n",
        "    s = s.translate(str.maketrans({\"'\" : \" \"}))\n",
        "\n",
        "    # Obtenção dos tokens\n",
        "    tokens = word_tokenize(s)\n",
        "\n",
        "\n",
        "    words = [word for word in tokens if not word in stop_words and not word.isdigit()] # remove stopwords e dígitos\n",
        "\n",
        "    # Lemmatizar os tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = list(map(lemmatizer.lemmatize, words))\n",
        "    domain_stopwords = list(map(lemmatizer.lemmatize, domain_stopwords))\n",
        "\n",
        "    words = [word for word in words if not word in domain_stopwords] # remove palavras de domínio\n",
        "\n",
        "\n",
        "    return \" \".join(words).strip()"
      ],
      "metadata": {
        "id": "M3l91Isc5jZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo a bag-of-words\n",
        "def compute_bag_of_words(dataset, lang, domain_stopwords=[]):\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X = vectorizer.fit_transform(dataset['code0'])\n",
        "\n",
        "    count_vect_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
        "\n",
        "    return count_vect_df, vectorizer"
      ],
      "metadata": {
        "id": "KQFIiKZg5zZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "AmpCS_yDEIOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_train_df, vectorizer = compute_bag_of_words(data_train,'english')\n",
        "bow_test_df = pd.DataFrame(vectorizer.transform(data_test['opinion']).todense(), columns=vectorizer.get_feature_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "hcBhosKa-fNo",
        "outputId": "f858a795-39c8-4f29-a932-a7d916d2e4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'code0'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'code0'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2400299100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbow_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'opinion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2745677243.py\u001b[0m in \u001b[0;36mcompute_bag_of_words\u001b[0;34m(dataset, lang, domain_stopwords)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcount_vect_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'code0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregar conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "\n",
        "# Certifique-se de que todas as colunas são do tipo string\n",
        "train_data['response'] = train_data['response'].astype(str)\n",
        "train_data['code0'] = train_data['code0'].astype(str)\n",
        "\n",
        "test_data['response'] = test_data['response'].astype(str)\n",
        "test_data['code0'] = test_data['code0'].astype(str)\n",
        "\n",
        "# Dividir o conjunto de dados em treino e teste\n",
        "# (Você pode ajustar o tamanho do conjunto de teste conforme necessário)\n",
        "# Mantive o mesmo random_state para reprodução consistente\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train_data['response'], train_data['code0'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Vetorização usando TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Previsões\n",
        "predictions = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f'Acurácia do modelo: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "7CQLQC6QEzoT",
        "outputId": "a6c1d6f7-c9d0-4ea8-842f-028e183980c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'code0'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'code0'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2963777369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Certifique-se de que todas as colunas são do tipo string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'code0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Carregar conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "\n",
        "# Certifique-se de que todas as colunas são do tipo string\n",
        "train_data['response'] = train_data['response'].astype(str)\n",
        "train_data['code0'] = train_data['code0'].astype(str)\n",
        "\n",
        "test_data['response'] = test_data['response'].astype(str)\n",
        "test_data['code0'] = test_data['code0'].astype(str)\n",
        "\n",
        "# Dividir o conjunto de dados em treino e teste\n",
        "# (Você pode ajustar o tamanho do conjunto de teste conforme necessário)\n",
        "# Mantive o mesmo random_state para reprodução consistente\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train_data['response'], train_data['code0'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Vetorização usando TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Previsões\n",
        "predictions = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "\n",
        "# Matriz de Confusão\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(f'Acurácia do modelo: {accuracy}')\n",
        "print(f'Precisão do modelo: {precision}')\n",
        "print(f'Revocação do modelo: {recall}')\n",
        "print(f'Medida F do modelo: {f1}')\n",
        "\n",
        "# Mostrar a matriz de confusão\n",
        "print('\\nMatriz de Confusão:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "Ua8x1ZDQQiip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Carregar conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "\n",
        "# Certifique-se de que todas as colunas são do tipo string\n",
        "train_data['response'] = train_data['response'].astype(str)\n",
        "train_data['code0'] = train_data['code0'].astype(str)\n",
        "\n",
        "test_data['response'] = test_data['response'].astype(str)\n",
        "test_data['code0'] = test_data['code0'].astype(str)\n",
        "\n",
        "# Vetorização usando TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['response'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['response'])\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_vectorized, train_data['code0'])\n",
        "\n",
        "# Previsões\n",
        "predictions = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(test_data['code0'], predictions)\n",
        "precision = precision_score(test_data['code0'], predictions, average='weighted')\n",
        "recall = recall_score(test_data['code0'], predictions, average='weighted')\n",
        "f1 = f1_score(test_data['code0'], predictions, average='weighted')\n",
        "\n",
        "# Matriz de Confusão\n",
        "conf_matrix = confusion_matrix(test_data['code0'], predictions)\n",
        "\n",
        "print(f'Acurácia do modelo: {accuracy}')\n",
        "print(f'Precisão do modelo: {precision}')\n",
        "print(f'Revocação do modelo: {recall}')\n",
        "print(f'Medida F do modelo: {f1}')\n",
        "\n",
        "# Mostrar a matriz de confusão\n",
        "print('\\nMatriz de Confusão:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "cNBrw71HBFfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Baixe as stopwords e o modelo do NLTK (se necessário)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Carregue os conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "\n",
        "# Função de pré-processamento\n",
        "def preprocess_text(text):\n",
        "    # Converte para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove pontuações\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Tokenização\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lematização\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstroi o texto\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Aplica o pré-processamento aos conjuntos de treino e teste\n",
        "train_data['processed_response'] = train_data['response'].apply(preprocess_text)\n",
        "test_data['processed_response'] = test_data['response'].apply(preprocess_text)\n",
        "\n",
        "# Visualiza as alterações\n",
        "print(\"Exemplo de texto original:\")\n",
        "print(train_data['response'].iloc[0])\n",
        "print(\"\\nExemplo de texto pré-processado:\")\n",
        "print(train_data['processed_response'].iloc[0])\n",
        "\n",
        "# Vetorização usando CountVectorizer para bag of words\n",
        "vectorizer = CountVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['response'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['response'])\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_vectorized, train_data['code0'])\n",
        "\n",
        "# Previsões\n",
        "predictions = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(test_data['code0'], predictions)\n",
        "precision = precision_score(test_data['code0'], predictions, average='weighted')\n",
        "recall = recall_score(test_data['code0'], predictions, average='weighted')\n",
        "f1 = f1_score(test_data['code0'], predictions, average='weighted')\n",
        "\n",
        "# Matriz de Confusão\n",
        "conf_matrix = confusion_matrix(test_data['code0'], predictions)\n",
        "\n",
        "print(f'Acurácia do modelo: {accuracy}')\n",
        "print(f'Precisão do modelo: {precision}')\n",
        "print(f'Revocação do modelo: {recall}')\n",
        "print(f'Medida F do modelo: {f1}')\n",
        "\n",
        "# Mostrar a matriz de confusão\n",
        "print('\\nMatriz de Confusão:')\n",
        "print(conf_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "SVCs7v1uCFN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Carregar conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "\n",
        "# Certifique-se de que todas as colunas são do tipo string\n",
        "train_data['response'] = train_data['response'].astype(str)\n",
        "train_data['code0'] = train_data['code0'].astype(str)\n",
        "\n",
        "test_data['response'] = test_data['response'].astype(str)\n",
        "test_data['code0'] = test_data['code0'].astype(str)\n",
        "\n",
        "# Vetorização usando CountVectorizer para bag of words\n",
        "vectorizer = CountVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['response'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['response'])\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train_vectorized, train_data['code0'])\n",
        "\n",
        "# Previsões\n",
        "predictions = svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(test_data['code0'], predictions)\n",
        "precision = precision_score(test_data['code0'], predictions, average='weighted')\n",
        "recall = recall_score(test_data['code0'], predictions, average='weighted')\n",
        "f1 = f1_score(test_data['code0'], predictions, average='weighted')\n",
        "\n",
        "# Matriz de Confusão\n",
        "conf_matrix = confusion_matrix(test_data['code0'], predictions)\n",
        "\n",
        "print(f'Acurácia do modelo: {accuracy}')\n",
        "print(f'Precisão do modelo: {precision}')\n",
        "print(f'Revocação do modelo: {recall}')\n",
        "print(f'Medida F do modelo: {f1}')\n",
        "\n",
        "# Mostrar a matriz de confusão\n",
        "print('\\nMatriz de Confusão:')\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "S6T3pGU3BeG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Baixe as stopwords e o modelo do NLTK (se necessário)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Carregue os conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\",  encoding=\"iso-8859-1\", sep = ';')\n",
        "\n",
        "# Função de pré-processamento\n",
        "def preprocess_text(text):\n",
        "    # Converte para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove pontuações\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Tokenização\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lematização\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstroi o texto\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Aplica o pré-processamento aos conjuntos de treino e teste\n",
        "train_data['processed_response'] = train_data['response'].apply(preprocess_text)\n",
        "test_data['processed_response'] = test_data['response'].apply(preprocess_text)\n",
        "\n",
        "# Vetorização usando CountVectorizer para bag of words\n",
        "vectorizer = CountVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['processed_response'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['processed_response'])\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "\n",
        "# Realiza validação cruzada\n",
        "cv_scores = cross_val_score(svm_model, X_train_vectorized, train_data['code0'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Exibe os resultados da validação cruzada\n",
        "print(\"Acurácia média na validação cruzada: {:.2f}%\".format(100 * cv_scores.mean()))\n"
      ],
      "metadata": {
        "id": "g4v35Jl0CogL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Baixe as stopwords e o modelo do NLTK (se necessário)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Carregue os conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep = ';')\n",
        "\n",
        "# Função de pré-processamento\n",
        "def preprocess_text(text):\n",
        "    # Converte para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove pontuações\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Tokenização\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lematização\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstroi o texto\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Aplica o pré-processamento aos conjuntos de treino e teste\n",
        "train_data['processed_response'] = train_data['response'].apply(preprocess_text)\n",
        "test_data['processed_response'] = test_data['response'].apply(preprocess_text)\n",
        "\n",
        "# Vetorização usando CountVectorizer para bag of words\n",
        "vectorizer = CountVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['processed_response'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['processed_response'])\n",
        "\n",
        "# Parâmetros para Grid Search\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "\n",
        "# Grid Search para encontrar os melhores parâmetros\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_vectorized, train_data['code0_3'])\n",
        "\n",
        "# Melhores parâmetros encontrados\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Melhores parâmetros encontrados:\", best_params)\n",
        "\n",
        "# Avaliação do modelo usando os melhores parâmetros\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "predictions = best_svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Avaliação do modelo\n",
        "accuracy = accuracy_score(test_data['code0_3'], predictions)\n",
        "precision = precision_score(test_data['code0_3'], predictions, average='weighted')\n",
        "recall = recall_score(test_data['code0_3'], predictions, average='weighted')\n",
        "f1 = f1_score(test_data['code0_3'], predictions, average='weighted')\n",
        "\n",
        "# Exibe as métricas de avaliação\n",
        "print(\"Acurácia do modelo: {:.2f}%\".format(100 * accuracy))\n",
        "print(\"Precisão do modelo: {:.2f}\".format(precision))\n",
        "print(\"Revocação do modelo: {:.2f}\".format(recall))\n",
        "print(\"Medida F do modelo: {:.2f}\".format(f1))\n"
      ],
      "metadata": {
        "id": "16eiLN2TDMcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Baixe as stopwords e o modelo do NLTK (se necessário)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Carregue os conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep=';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep=';')\n",
        "\n",
        "# Função de pré-processamento\n",
        "def preprocess_text(text):\n",
        "    # Converte para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove pontuações\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Tokenização\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lematização\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstroi o texto\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Aplica o pré-processamento aos conjuntos de treino e teste\n",
        "train_data['processed_response'] = train_data['response'].apply(preprocess_text)\n",
        "test_data['processed_response'] = test_data['response'].apply(preprocess_text)\n",
        "\n",
        "# Vetorização usando CountVectorizer para bag of words\n",
        "vectorizer = CountVectorizer(max_features=1000)  # Ajuste o número de features conforme necessário\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['processed_response'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['processed_response'])\n",
        "\n",
        "# Parâmetros para Grid Search\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "\n",
        "# Grid Search para encontrar os melhores parâmetros\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_vectorized, train_data['code0_3'])\n",
        "\n",
        "# Melhores parâmetros encontrados\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Melhores parâmetros encontrados:\", best_params)\n",
        "\n",
        "# Avaliação do modelo usando os melhores parâmetros\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "predictions = best_svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Avaliação geral\n",
        "accuracy = accuracy_score(test_data['code0_3'], predictions)\n",
        "precision = precision_score(test_data['code0_3'], predictions, average='weighted')\n",
        "recall = recall_score(test_data['code0_3'], predictions, average='weighted')\n",
        "f1 = f1_score(test_data['code0_3'], predictions, average='weighted')\n",
        "\n",
        "print(\"\\nMétricas gerais do modelo:\")\n",
        "print(\"Acurácia do modelo: {:.2f}%\".format(100 * accuracy))\n",
        "print(\"Precisão do modelo: {:.2f}\".format(precision))\n",
        "print(\"Revocação do modelo: {:.2f}\".format(recall))\n",
        "print(\"Medida F do modelo: {:.2f}\".format(f1))\n",
        "\n",
        "# --- NOVO: Relatório detalhado por classe ---\n",
        "print(\"\\nRelatório por classe:\")\n",
        "print(classification_report(test_data['code0_3'], predictions, digits=2))\n",
        "\n",
        "# --- NOVO: % de acerto por classe ---\n",
        "print(\"\\nPercentual de acerto por classe:\")\n",
        "classes = sorted(test_data['code0_3'].unique())\n",
        "for cls in classes:\n",
        "    # Seleciona apenas os exemplos da classe verdadeira == cls\n",
        "    mask = test_data['code0_3'] == cls\n",
        "    acc_cls = accuracy_score(test_data['code0_3'][mask], predictions[mask])\n",
        "    print(f\"Classe {cls}: {acc_cls * 100:.2f}% de acerto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "30Ob1pSYlE4H",
        "outputId": "81e9fa30-6ad0-4be6-eab2-56239827e76b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-607092967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Aplica o pré-processamento aos conjuntos de treino e teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_response'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_response'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-607092967.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Tokenização\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Baixe os recursos do NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # 👈 necessário para corrigir o erro\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Carregue os conjuntos de dados\n",
        "train_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep=';')\n",
        "test_data = pd.read_csv(\"caminho do arquivo\", encoding=\"iso-8859-1\", sep=';')\n",
        "\n",
        "# Função de pré-processamento\n",
        "def preprocess_text(text):\n",
        "    # Converte para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove pontuações\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Tokenização\n",
        "    tokens = word_tokenize(text, language=\"portuguese\")\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lematização\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Reconstroi o texto\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Aplica o pré-processamento\n",
        "train_data['processed_response'] = train_data['response'].apply(preprocess_text)\n",
        "test_data['processed_response'] = test_data['response'].apply(preprocess_text)\n",
        "\n",
        "# Vetorização\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X_train_vectorized = vectorizer.fit_transform(train_data['processed_response'])\n",
        "X_test_vectorized = vectorizer.transform(test_data['processed_response'])\n",
        "\n",
        "# Parâmetros para Grid Search\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly']}\n",
        "\n",
        "# Modelo SVM\n",
        "svm_model = SVC()\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_vectorized, train_data['code0_3'])\n",
        "\n",
        "# Melhores parâmetros encontrados\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Melhores parâmetros encontrados:\", best_params)\n",
        "\n",
        "# Avaliação\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "predictions = best_svm_model.predict(X_test_vectorized)\n",
        "\n",
        "# Salva rótulos verdadeiros e predições em CSV\n",
        "results_df = pd.DataFrame({\n",
        "    'true_label': test_data['code0_3'],\n",
        "    'predicted_label': predictions\n",
        "})\n",
        "\n",
        "# Caminho e salvamento\n",
        "results_df.to_csv(\"/content/svm_predictions.csv\", index=False, encoding='utf-8')\n",
        "print(\"\\nArquivo 'svm_predictions.csv' criado com sucesso!\")\n",
        "\n",
        "\n",
        "# Métricas gerais\n",
        "accuracy = accuracy_score(test_data['code0_3'], predictions)\n",
        "precision = precision_score(test_data['code0_3'], predictions, average='weighted')\n",
        "recall = recall_score(test_data['code0_3'], predictions, average='weighted')\n",
        "f1 = f1_score(test_data['code0_3'], predictions, average='weighted')\n",
        "\n",
        "print(\"\\nMétricas gerais do modelo:\")\n",
        "print(\"Acurácia: {:.2f}%\".format(100 * accuracy))\n",
        "print(\"Precisão: {:.2f}\".format(precision))\n",
        "print(\"Revocação: {:.2f}\".format(recall))\n",
        "print(\"F1: {:.2f}\".format(f1))\n",
        "\n",
        "# Relatório detalhado\n",
        "print(\"\\nRelatório por classe:\")\n",
        "print(classification_report(test_data['code0_3'], predictions, digits=2))\n",
        "\n",
        "# % de acerto por classe\n",
        "print(\"\\nPercentual de acerto por classe:\")\n",
        "classes = sorted(test_data['code0_3'].unique())\n",
        "for cls in classes:\n",
        "    mask = test_data['code0_3'] == cls\n",
        "    acc_cls = accuracy_score(test_data['code0_3'][mask], predictions[mask])\n",
        "    print(f\"Classe {cls}: {acc_cls * 100:.2f}% de acerto\")\n",
        "\n",
        "# Matriz de confusão\n",
        "cm = confusion_matrix(test_data['code0_3'], predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title(\"Matriz de Confusão\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o5me4s27ls6w",
        "outputId": "7c5082f9-3791-4b16-9280-56ad218ffb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhores parâmetros encontrados: {'C': 1, 'kernel': 'rbf'}\n",
            "\n",
            "Métricas gerais do modelo:\n",
            "Acurácia: 62.56%\n",
            "Precisão: 0.63\n",
            "Revocação: 0.63\n",
            "F1: 0.63\n",
            "\n",
            "Relatório por classe:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.64      0.64       563\n",
            "           1       0.67      0.66      0.66       722\n",
            "           2       0.53      0.51      0.52       315\n",
            "\n",
            "    accuracy                           0.63      1600\n",
            "   macro avg       0.61      0.61      0.61      1600\n",
            "weighted avg       0.63      0.63      0.63      1600\n",
            "\n",
            "\n",
            "Percentual de acerto por classe:\n",
            "Classe 0: 64.30% de acerto\n",
            "Classe 1: 66.07% de acerto\n",
            "Classe 2: 51.43% de acerto\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}