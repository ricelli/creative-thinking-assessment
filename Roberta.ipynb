{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68nsnX8ekxtH",
        "outputId": "7b1d2440-32bd-4111-d535-02c641b5bf4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
        "\n",
        "# Carregar dados do arquivo CSV\n",
        "file_path = \"caminho do arquivo\"  # Certifique-se de ajustar o caminho conforme necessário\n",
        "df = pd.read_csv(file_path, delimiter=';', encoding='utf-8')\n",
        "\n",
        "# Escolher as colunas relevantes\n",
        "df = df[['response', 'code0']]\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenizar textos usando o tokenizer do RoBERTa\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_tokens = tokenizer(list(train_data['response']), padding=True, truncation=True, return_tensors='tf')\n",
        "test_tokens = tokenizer(list(test_data['response']), padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Carregar o modelo pré-treinado do RoBERTa\n",
        "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Criar o modelo de classificação com camadas adicionais usando a API funcional do Keras\n",
        "inputs = Input(shape=(None,), dtype=tf.int32)\n",
        "roberta_outputs = roberta_model(inputs)['pooler_output']\n",
        "output = Dense(1, activation='sigmoid')(roberta_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "labels = np.array(train_data['code0'])\n",
        "model.fit(train_tokens['input_ids'], labels, epochs=3, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Avaliar o modelo no conjunto de teste\n",
        "test_labels = np.array(test_data['code0'])\n",
        "loss, accuracy = model.evaluate(test_tokens['input_ids'], test_labels)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mSG1h5Cobq7",
        "outputId": "a56ae324-f16f-420f-cd3e-d769c2389262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        }
      ]
    }
  ]
}